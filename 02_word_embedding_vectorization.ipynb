{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8scpvN5oJwGS/DncHeQs6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 단어 벡터화와 임베딩 실습\n",
        "\n",
        "단어를 수치로 표현하는 방법 중 `단어 벡터화(Bag of Words, TF-IDF)`와 `단어 임베딩(Word Embedding)`의 차이를 이해하고, 임베딩을 통해 문장의 의미를 잘 보존하면서 벡터화하는 방법을 실습할 예정입니다.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. 단어 벡터화 방식 정리\n",
        "\n",
        "단어 벡터화는 텍스트를 숫자 벡터로 변환하는 전처리 과정입니다. 대표적인 방법은 다음과 같습니다.\n",
        "\n",
        "- **Bag of Words (BoW)**: 단어의 등장 여부나 빈도만 반영. 의미 파악 불가.\n",
        "- **TF-IDF**: 빈도에 가중치를 주는 방식이지만 여전히 단어 간 의미 관계는 반영하지 않음.\n",
        "- 이들은 모두 **희소 벡터(Sparse Vector)**를 생성합니다.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. 단어 임베딩 개념 소개\n",
        "\n",
        "임베딩(Embedding)은 단어를 **의미 기반의 밀집 벡터(Dense Vector)**로 표현하는 방식입니다.\n",
        "대표적인 특징:\n",
        "\n",
        "- 유사한 의미의 단어는 벡터 공간에서 가까운 위치에 있음\n",
        "- 연산 가능 (예: king - man + woman ≈ queen)\n",
        "- 대표 모델: Word2Vec, GloVe, FastText 등\n",
        "\n",
        "이번 실습에서는 TensorFlow의 `TextVectorization`과 `Embedding` 레이어를 활용합니다.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. 시퀀스 인코딩과 패딩 처리\n",
        "\n",
        "문장을 정수 인덱스로 변환한 후, 모든 문장의 길이를 맞추기 위해 패딩을 적용합니다.\n",
        "\n",
        "- 예시: \"샐러드 맛이 깔끔해서 좋아요\" → `[82, 53, 75, 29, 0, 0, 0, 0, 0, 0]`\n",
        "- 최대 시퀀스 길이 (`max_length`)는 실습에서 설정\n",
        "- 인코딩 시 단어 등장 빈도에 따라 낮은 번호가 할당됨 (자주 등장하는 단어가 앞 번호)\n",
        "\n",
        "---\n",
        "\n",
        "## 4. 임베딩 벡터 생성\n",
        "\n",
        "`Embedding` 레이어를 통해 각 정수 인덱스를 **고정된 차원의 밀집 벡터**로 매핑합니다.\n",
        "\n",
        "- 입력: `[82, 53, 75, 29, ...]`\n",
        "- 출력: 각 인덱스에 해당하는 `n차원 임베딩 벡터`\n",
        "- 실습에서는 4차원 임베딩 사용\n",
        "\n",
        "---\n",
        "\n",
        "## 5. 차원 축소 및 시각화\n",
        "\n",
        "임베딩된 벡터는 고차원이므로(4개의 차원으로 되어 있으면 시각화하기가 어렵기 때문에) 시각화를 위해 2차원으로 축소합니다.\n",
        "\n",
        "- 차원 축소 기법: `TSNE`, `UMAP`, `PCA` 중 선택\n",
        "- 유사한 문장은 2차원 평면에서 가까운 위치에 배치됨\n",
        "- 예: ‘맛없어요’ 문장들끼리 가까이, ‘맛있어요’는 멀리\n",
        "\n",
        "---\n",
        "\n",
        "## 6. TensorBoard로 임베딩 시각화\n",
        "\n",
        "학습된 임베딩 벡터를 **TensorBoard의 Embedding Projector**를 통해 확인할 수 있습니다.\n",
        "\n",
        "- 코사인 유사도/유클리디언 거리 기준으로 단어 간 관계를 시각적으로 확인\n",
        "- 다양한 차원 축소 방식으로 탐색 가능\n",
        "- 딥러닝 모델을 학습시키고 나서, 그 모델이 얼마나 잘 예측했는지 Accuracy(정확도)까지 확인\n",
        "\n",
        "---\n",
        "\n",
        "## 정리\n",
        "\n",
        "| 항목 | 단어 벡터화 | 단어 임베딩 |\n",
        "|------|-------------|--------------|\n",
        "| 벡터 형태 | 희소 벡터 | 밀집 벡터 |\n",
        "| 의미 유사성 | 반영 어려움 | 반영 가능 |\n",
        "| 차원 수 | 큼 | 상대적으로 작음 |\n",
        "| 연산 가능성 | 제한적 | 유연한 연산 가능 |\n",
        "| 대표 방식 | BoW, TF-IDF | Word2Vec, GloVe, FastText |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "86z6SRMlD6jE"
      }
    }
  ]
}